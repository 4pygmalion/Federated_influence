{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code provide the estmation results (calibration) between the change of estimated loss without train data z and acutal loss without loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "\n",
    "from in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2class_mnist(num_a, num_b):\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data(path='mnist.npz')\n",
    "    x_train, x_test = x_train.reshape([-1, 784]) / 255.0, x_test.reshape([-1, 784]) / 255.0\n",
    "    x_train_2class, y_train_2class = from10to2classes(x_train, y_train, num_a, num_b)\n",
    "    x_test_2class, y_test_2class = from10to2classes(x_test, y_test, num_a, num_b)\n",
    "\n",
    "    return (x_train_2class, y_train_2class), (x_test_2class, y_test_2class)\n",
    "\n",
    "def from10to2classes(x, y, num_a, num_b):\n",
    "    is_num_a, is_num_b = y == num_a, y == num_b\n",
    "    x_2class = np.concatenate([x[is_num_a], x[is_num_b]])\n",
    "    y_2class = np.concatenate([np.ones(is_num_a.sum()), np.zeros(is_num_b.sum())]).reshape([-1, 1])\n",
    "\n",
    "    return x_2class, y_2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.007688167909587146, tol=1e-08)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_A, NUM_B = 1, 7\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = get_2class_mnist(NUM_A, NUM_B)\n",
    "train_sample_num = len(x_train)\n",
    "\n",
    "# prepare sklearn model to train w\n",
    "C = 1.0 / (train_sample_num * WEIGHT_DECAY)\n",
    "sklearn_model = linear_model.LogisticRegression(C=C, solver='lbfgs', tol=1e-8, fit_intercept=True)\n",
    "sklearn_model.fit(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt = sklearn_model.coef_.ravel()\n",
    "b_opt = sklearn_model.intercept_.ravel()\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(784,)))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid', use_bias=True))\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "w_ref = np.load('../cache/compare/w.npy')\n",
    "model.set_weights([w_ref.reshape(784, 1), b_opt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grad_z test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784,), (1,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_INDEX = 5\n",
    "\n",
    "x_test_tf = tf.convert_to_tensor(x_test[TEST_INDEX: TEST_INDEX+1])\n",
    "y_test_tf = tf.convert_to_tensor(y_test[TEST_INDEX: TEST_INDEX+1])\n",
    "\n",
    "test_grad_my = grad_z(x_test_tf, y_test_tf, f=model)\n",
    "\n",
    "# as vector\n",
    "test_grad_my_coef = test_grad_my[0].numpy().ravel() \n",
    "test_grad_my_bias = test_grad_my[1].numpy().ravel() \n",
    "test_grad_my_coef.shape, test_grad_my_bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### grad_z_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784,), (1,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "x_train_tf = tf.convert_to_tensor(x_train[0:2])\n",
    "y_train_tf = tf.convert_to_tensor(y_train[0:2])\n",
    "\n",
    "train_grad_my = grad_z(x_train_tf, y_train_tf, model, for_train=True)\n",
    "\n",
    "# as vector\n",
    "train_grad_my_coef = train_grad_my[0].numpy().ravel() \n",
    "train_grad_my_bias = train_grad_my[1].numpy().ravel() \n",
    "\n",
    "train_grad_my_coef.shape, train_grad_my_bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### hvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf = tf.convert_to_tensor(x_train)\n",
    "y_train_tf = tf.convert_to_tensor(y_train)\n",
    "x_test_tf = tf.convert_to_tensor(x_test[TEST_INDEX: TEST_INDEX+1])\n",
    "y_test_tf = tf.convert_to_tensor(y_test[TEST_INDEX: TEST_INDEX+1])\n",
    "\n",
    "test_grad_my = grad_z(x_test_tf, y_test_tf, f=model)\n",
    "\n",
    "    \n",
    "hvp_my = get_hessian_vector_product(x_train_tf, y_train_tf, model, test_grad_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train, Y train shape: (13007, 784) (13007, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train_tf = tf.convert_to_tensor(x_train)\n",
    "y_train_tf = tf.convert_to_tensor(y_train)\n",
    "\n",
    "print(\"X train, Y train shape:\", x_train_tf.shape, y_train_tf.shape)\n",
    "\n",
    "\n",
    "s_test_my = get_inv_hessian_vector_product(x_train_tf, y_train_tf, test_grad_my, model,\n",
    "                                            scale=10,\n",
    "                                            n_recursion=1000,\n",
    "                                            verbose=False)\n",
    "\n",
    "s_test_ref = np.load('../cache/compare/hvp.npy')  # List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/13007] Estimated loss diff: -8.192967044073954e-07\n",
      "[1001/13007] Estimated loss diff: -8.23931773716499e-07\n",
      "[2001/13007] Estimated loss diff: -1.5307424263917686e-06\n",
      "[3001/13007] Estimated loss diff: -6.768512706844903e-07\n",
      "[4001/13007] Estimated loss diff: 1.3205292248017093e-06\n",
      "[5001/13007] Estimated loss diff: -7.239504803624082e-07\n",
      "[6001/13007] Estimated loss diff: -8.207535633255314e-07\n",
      "[7001/13007] Estimated loss diff: -4.886687748484125e-07\n",
      "[8001/13007] Estimated loss diff: -6.688261282946529e-07\n",
      "[9001/13007] Estimated loss diff: -7.721387722919056e-07\n",
      "[10001/13007] Estimated loss diff: -7.959436515333711e-07\n",
      "[11001/13007] Estimated loss diff: -8.061566682804659e-07\n",
      "[12001/13007] Estimated loss diff: -4.749312495509612e-06\n",
      "[13001/13007] Estimated loss diff: -6.207566671236688e-07\n"
     ]
    }
   ],
   "source": [
    "from influence_utils import multiply_for_influe\n",
    "\n",
    "train_sample_num = len(x_train_tf)\n",
    "loss_diff_approx = np.zeros(train_sample_num)\n",
    "\n",
    "for i in range(train_sample_num):\n",
    "    \n",
    "    # Get train grad\n",
    "    train_grad = grad_z(x_train_tf[i: i+1], y_train[i: i+1], model, for_train=True)\n",
    "    loss_diff_approx[i] = multiply_for_influe(train_grad, s_test_my) / train_sample_num\n",
    "\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print('[{}/{}] Estimated loss diff: {}'.format(i+1, train_sample_num, loss_diff_approx[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### influence function calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test original loss\n",
    "\n",
    "TEST_INDEX = 5\n",
    "\n",
    "x_test_tf = tf.convert_to_tensor(x_test[TEST_INDEX: TEST_INDEX+1])\n",
    "y_test_tf = tf.convert_to_tensor(y_test[TEST_INDEX: TEST_INDEX+1])\n",
    "\n",
    "w_ref = sklearn_model.coef_.T.ravel()\n",
    "b_ref = sklearn_model.intercept_.ravel()\n",
    "model.set_weights([w_ref.reshape(784, 1), b_ref])\n",
    "\n",
    "\n",
    "y_hat = model.predict(x_test_tf)\n",
    "test_loss_ori = tf.keras.losses.binary_crossentropy(y_test_tf, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01705281]\n",
      "True loss diff      :0.00012713298201560974\n",
      "Estimated loss diff :2.4732500225926487e-05\n",
      "[2/50]\n",
      "LBFGS training took [21] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01700836]\n",
      "True loss diff      :8.269213140010834e-05\n",
      "Estimated loss diff :2.502097544226885e-05\n",
      "[3/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01706263]\n",
      "True loss diff      :0.00013695470988750458\n",
      "Estimated loss diff :2.511500672996388e-05\n",
      "[4/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01701976]\n",
      "True loss diff      :9.408965706825256e-05\n",
      "Estimated loss diff :2.534409489892489e-05\n",
      "[5/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.0170214]\n",
      "True loss diff      :9.572692215442657e-05\n",
      "Estimated loss diff :2.7381279650782607e-05\n",
      "[6/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01707639]\n",
      "True loss diff      :0.00015071779489517212\n",
      "Estimated loss diff :2.7726696181374747e-05\n",
      "[7/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.0170465]\n",
      "True loss diff      :0.00012082606554031372\n",
      "Estimated loss diff :2.7911290910174884e-05\n",
      "[8/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01705014]\n",
      "True loss diff      :0.00012446381151676178\n",
      "Estimated loss diff :2.872777891244292e-05\n",
      "[9/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01708949]\n",
      "True loss diff      :0.00016381405293941498\n",
      "Estimated loss diff :2.9266858376757925e-05\n",
      "[10/50]\n",
      "LBFGS training took [21] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01707718]\n",
      "True loss diff      :0.00015150569379329681\n",
      "Estimated loss diff :2.9695946833489264e-05\n",
      "[11/50]\n",
      "LBFGS training took [21] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01701873]\n",
      "True loss diff      :9.305961430072784e-05\n",
      "Estimated loss diff :2.9952387116603686e-05\n",
      "[12/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.0170043]\n",
      "True loss diff      :7.862970232963562e-05\n",
      "Estimated loss diff :3.0288710059490543e-05\n",
      "[13/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01703546]\n",
      "True loss diff      :0.00010979175567626953\n",
      "Estimated loss diff :3.202651659797319e-05\n",
      "[14/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01708997]\n",
      "True loss diff      :0.00016429834067821503\n",
      "Estimated loss diff :3.318748612077412e-05\n",
      "[15/50]\n",
      "LBFGS training took [21] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01707439]\n",
      "True loss diff      :0.00014871731400489807\n",
      "Estimated loss diff :3.453043086205528e-05\n",
      "[16/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.0170308]\n",
      "True loss diff      :0.00010512396693229675\n",
      "Estimated loss diff :3.5042999603127486e-05\n",
      "[17/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01704262]\n",
      "True loss diff      :0.00011694617569446564\n",
      "Estimated loss diff :3.621349013964347e-05\n",
      "[18/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01704965]\n",
      "True loss diff      :0.00012397952377796173\n",
      "Estimated loss diff :3.8918096578175256e-05\n",
      "[19/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01705844]\n",
      "True loss diff      :0.00013277120888233185\n",
      "Estimated loss diff :4.24793750068305e-05\n",
      "[20/50]\n",
      "LBFGS training took [21] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01703813]\n",
      "True loss diff      :0.00011246092617511749\n",
      "Estimated loss diff :4.273033633219836e-05\n",
      "[21/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01707427]\n",
      "True loss diff      :0.00014859624207019806\n",
      "Estimated loss diff :4.326786869980622e-05\n",
      "[22/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01704626]\n",
      "True loss diff      :0.0001205839216709137\n",
      "Estimated loss diff :4.7563737385909215e-05\n",
      "[23/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01718735]\n",
      "True loss diff      :0.0002616792917251587\n",
      "Estimated loss diff :7.234203681536492e-05\n",
      "[24/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01717001]\n",
      "True loss diff      :0.00024433620274066925\n",
      "Estimated loss diff :7.50639200144125e-05\n",
      "[25/50]\n",
      "LBFGS training took [21] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.0171959]\n",
      "True loss diff      :0.0002702288329601288\n",
      "Estimated loss diff :0.00012189797078141648\n",
      "[26/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.0165051]\n",
      "True loss diff      :-0.000420568510890007\n",
      "Estimated loss diff :-0.00031266994147111186\n",
      "[27/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01675697]\n",
      "True loss diff      :-0.00016869790852069855\n",
      "Estimated loss diff :-0.00020457040929515696\n",
      "[28/50]\n",
      "LBFGS training took [21] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01673479]\n",
      "True loss diff      :-0.00019088014960289001\n",
      "Estimated loss diff :-0.0001841352997198638\n",
      "[29/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01668909]\n",
      "True loss diff      :-0.00023658014833927155\n",
      "Estimated loss diff :-0.00016267611165073087\n",
      "[30/50]\n",
      "LBFGS training took [21] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01685566]\n",
      "True loss diff      :-7.001683115959167e-05\n",
      "Estimated loss diff :-0.00014276656071503873\n",
      "[31/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01678534]\n",
      "True loss diff      :-0.00014032982289791107\n",
      "Estimated loss diff :-0.00013416861304590134\n",
      "[32/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01681432]\n",
      "True loss diff      :-0.00011135637760162354\n",
      "Estimated loss diff :-0.00013341740053855934\n",
      "[33/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01682323]\n",
      "True loss diff      :-0.00010244548320770264\n",
      "Estimated loss diff :-0.00012839739486211963\n",
      "[34/50]\n",
      "LBFGS training took [21] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01681498]\n",
      "True loss diff      :-0.00011068955063819885\n",
      "Estimated loss diff :-0.0001161149059625448\n",
      "[35/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01685262]\n",
      "True loss diff      :-7.304735481739044e-05\n",
      "Estimated loss diff :-0.00010879986670360564\n",
      "[36/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01677352]\n",
      "True loss diff      :-0.00015215016901493073\n",
      "Estimated loss diff :-9.710672090593745e-05\n",
      "[37/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01688403]\n",
      "True loss diff      :-4.164688289165497e-05\n",
      "Estimated loss diff :-9.646264178459435e-05\n",
      "[38/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01684335]\n",
      "True loss diff      :-8.232146501541138e-05\n",
      "Estimated loss diff :-9.640810095123758e-05\n",
      "[39/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01686166]\n",
      "True loss diff      :-6.401538848876953e-05\n",
      "Estimated loss diff :-9.62074317609154e-05\n",
      "[40/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01688124]\n",
      "True loss diff      :-4.443526268005371e-05\n",
      "Estimated loss diff :-9.096215141903771e-05\n",
      "[41/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01687384]\n",
      "True loss diff      :-5.182996392250061e-05\n",
      "Estimated loss diff :-9.008406953513966e-05\n",
      "[42/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.0168676]\n",
      "True loss diff      :-5.807355046272278e-05\n",
      "Estimated loss diff :-8.889835835089735e-05\n",
      "[43/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01690548]\n",
      "True loss diff      :-2.0187348127365112e-05\n",
      "Estimated loss diff :-8.701667461111515e-05\n",
      "[44/50]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01687524]\n",
      "True loss diff      :-5.0436705350875854e-05\n",
      "Estimated loss diff :-8.627587369872778e-05\n",
      "[45/50]\n",
      "LBFGS training took [21] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01689106]\n",
      "True loss diff      :-3.4615397453308105e-05\n",
      "Estimated loss diff :-8.438059337969296e-05\n",
      "[46/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01686681]\n",
      "True loss diff      :-5.8863312005996704e-05\n",
      "Estimated loss diff :-8.206564408651165e-05\n",
      "[47/50]\n",
      "LBFGS training took [21] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.0169227]\n",
      "True loss diff      :-2.9709190130233765e-06\n",
      "Estimated loss diff :-7.817268694075833e-05\n",
      "[48/50]\n",
      "LBFGS training took [23] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01692634]\n",
      "True loss diff      :6.668269634246826e-07\n",
      "Estimated loss diff :-7.80373777353013e-05\n",
      "[49/50]\n",
      "LBFGS training took [21] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01691039]\n",
      "True loss diff      :-1.527741551399231e-05\n",
      "Estimated loss diff :-7.749605098180917e-05\n",
      "[50/50]\n",
      "LBFGS training took [22] iter.\n",
      "Original loss       :[0.01692567]\n",
      "Retrain loss        :[0.01685287]\n",
      "True loss diff      :-7.280521094799042e-05\n",
      "Estimated loss diff :-7.583569995059382e-05\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_NUM = 50 \n",
    "\n",
    "# get high and low loss diff indice\n",
    "sorted_indice = np.argsort(loss_diff_approx)\n",
    "sample_indice = np.concatenate([sorted_indice[-int(SAMPLE_NUM/2):], sorted_indice[:int(SAMPLE_NUM/2)]])\n",
    "\n",
    "\n",
    "# calculate true loss diff\n",
    "loss_diff_true = np.zeros(SAMPLE_NUM)\n",
    "for i, index in zip(range(SAMPLE_NUM), sample_indice):\n",
    "    print('[{}/{}]'.format(i+1, SAMPLE_NUM))\n",
    "\n",
    "    # get minus one dataset\n",
    "    x_train_minus_one = np.delete(x_train, index, axis=0)\n",
    "    y_train_minus_one = np.delete(y_train, index, axis=0)\n",
    "\n",
    "    # retrain\n",
    "    C = 1.0 / ((train_sample_num - 1) * WEIGHT_DECAY)\n",
    "    sklearn_model_minus_one = linear_model.LogisticRegression(C=C, fit_intercept=False, tol=1e-8, solver='lbfgs')\n",
    "    sklearn_model_minus_one.fit(x_train_minus_one, y_train_minus_one.ravel())\n",
    "    print('LBFGS training took {} iter.'.format(sklearn_model_minus_one.n_iter_))\n",
    "\n",
    "    # Reference model (deep learning)\n",
    "    w_ref = sklearn_model_minus_one.coef_.T.ravel()\n",
    "    b_ref = sklearn_model_minus_one.intercept_.ravel()\n",
    "    model.set_weights([w_ref.reshape(784, 1), b_ref])\n",
    "    \n",
    "\n",
    "    # get retrain loss\n",
    "    x_test_tf = tf.convert_to_tensor(x_test[TEST_INDEX: TEST_INDEX+1])\n",
    "    y_test_tf = tf.convert_to_tensor(y_test[TEST_INDEX: TEST_INDEX+1])\n",
    "    y_hat = model(x_test_tf)\n",
    "    test_loss_retrain = tf.keras.losses.binary_crossentropy(y_test_tf, y_hat)\n",
    "\n",
    "    # get true loss diff\n",
    "    loss_diff_true[i] = test_loss_retrain.numpy() - test_loss_ori.numpy()\n",
    "\n",
    "    print('Original loss       :{}'.format(test_loss_ori))\n",
    "    print('Retrain loss        :{}'.format(test_loss_retrain))\n",
    "    print('True loss diff      :{}'.format(loss_diff_true[i]))\n",
    "    print('Estimated loss diff :{}'.format(loss_diff_approx[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAFNCAYAAADb4bKNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA8r0lEQVR4nO3de3xV9Znv8c9DSCAQ7pcAgZBkJwRQETWt2NZetI6XXtReplpHbW1ray/OOO3U9nTaWqc9vb7GOU5v06lOtVVRW62eqdWj1Y5ovRQUFVFIdi6QBEIg4RIISfbOc/7YK3ETdkJC9iWX7/v12q+s/Vtr//azFpCH9Vu/9Sxzd0RERNJhQqYDEBGR8UNJR0RE0kZJR0RE0kZJR0RE0kZJR0RE0kZJR0RE0kZJR2SMM7OPmdlTce/bzKwkWM41s/9rZvvM7N6g7dtmttvMdmYqZhm7JmY6AJGxysxqgU+6+2OZjiWeu+fFvf0QkA/McfeImRUCXwSWuvuujAQoY5rOdETGt6XAVnePBO8LgT1KOJIqSjoiaWZmk8zs38ysMXj9m5lNCtbNNbP/NrO9ZtZiZuvMbEKw7nozazCzA2a2xczO7qf/OWb2oJntN7PngVCf9W5mpWb2LeAbwEeCIbdPA48Ci4L3v0rlcZDxScNrIun3NWANsBpw4AHgn4GvExvaqgfmBduuAdzMyoHPA29y90YzKwKy+un/J8BhYCFQDDwC1PTdyN2/aWYOlLr73wGY2RbgN+6+ePi7KXI0nemIpN9lwI3uvsvdm4FvAZcH67qIJYul7t7l7us8ViAxCkwCVppZtrvXunu4b8dmlgV8EPiGux90903AbenYKZHBUNIRSb9FQF3c+7qgDeCHQBXw/8ys2sy+AuDuVcA/ADcAu8xsrZkt4mjziI1gbO/Tv8iIoKQjkn6NxC7g9ygM2nD3A+7+RXcvAd4P/GPPtRt3v9Pd3xZ81oHvJ+i7GYgAS/r0LzIiKOmIpFa2mU2Oe00E7gL+2czmmdlcYhfzfwNgZu8NLvIbsI/YsFq3mZWb2VnBhIPDQDvQ3ffL3D0K3AfcYGZTzGwlcGVa9lRkEJR0RFLrIWIJoud1A/BtYD3wMvAK8ELQBlAGPAa0Ac8AP3X3J4hdz/kesBvYCcwHvtrPd34eyAu2+xXwX8ndJZHjZ3qIm4iIpIvOdEREJG2UdEREJG2UdEREJG2UdEREJG2UdEREJG1Ue20Ac+fO9aKiokyHISIy4rk7HR0ddHd389prr+1293mJtlPSGUBRURHr16/PdBgiIiNae3s74XCYzs5Oli5dyty5c/stvaSkIyIix621tZXa2lqysrIoLy9n6tSpA26vpCMiIselsbGRHTt2MHXqVEKhENnZ2cf8jJKOiIgMSTQapba2lr179zJnzhyWLl1KrFzgsSnpiIjIoHV0dFBVVUVHRwdLlixh/vz5Q/q8ko6IiAzK/v37qa6uxswoKytj2rRpQ+5DSUdERI6pqamJ+vp6cnNzCYVCTJo06bj6UdIREZF+dXd3U1dXR0tLC7NmzaKoqIgJE46/roCSjoiIJNTZ2Uk4HObQoUMsWrSIhQsXDrtPJR0RETlKW1sb1dXVRKNRQqEQM2fOTEq/SjoiInKE3bt3s23bNnJycigrKyM3NzdpfSvpiIgIEKuftn37dpqbm5k+fTrFxcVMnJjcNKGkIyIiRCIRqqurOXDgAPn5+RQUFAz6hs+hUNIRERnnDh06RDgcpquri+LiYmbPnp2y71LSEREZx4ZasHO4lHRERMapnoKdeXl5lJSUDKpg53Ap6YiIjDPRaJSamhr27dvH3LlzKSwsTMn1m0SUdERExpHDhw8TDofp6OigsLCQefMSPuAzZZR0RETGiWQU7Byu4y+gkwRmdp6ZbTGzKjP7SoL1k8zs7mD9c2ZWFLfuq0H7FjM7dwh93mxmbSnbKRGREaipqYnKykpycnJYsWJFRhIOZPBMx8yygJ8A5wD1wF/N7EF33xy32SeAVncvNbNLgO8DHzGzlcAlwAnAIuAxM1sWfKbfPs2sApiVht0TERkRkl2wc7gyeabzZqDK3avdvRNYC1zYZ5sLgduC5d8CZ1vsateFwFp373D3GqAq6K/fPoMk90PgyyneLxGREaGzs5MtW7bQ0tJCQUEBJSUlGU04kNlrOgXA9rj39cDp/W3j7hEz2wfMCdqf7fPZgmC5vz4/Dzzo7jvSNUtDRCRT2traCIfDuDulpaXMmDEj0yEB42QigZktAj4MvHMQ214NXA1QWFiY2sBERFIgvmBnaWkpkydPznRIvTJ5ntUALIl7vzhoS7iNmU0EZgB7Bvhsf+2nAKVAlZnVAlPMrCpRUO7+C3evcPeKdE8lFBEZDndn27Zt1NXVMW3aNFasWDGiEg5k9kznr0CZmRUTSwyXAB/ts82DwJXAM8CHgMfd3c3sQeBOM/tXYhMJyoDnAUvUp7u/Cizo6dTM2ty9NKV7JyKSRpFIhHA4TFtbGwsWLGDRokVpu+FzKDKWdIJrNJ8HHgGygFvd/VUzuxFY7+4PArcAvw7OSlqIJRGC7e4BNgMR4HPuHgVI1Ge6901EJJ16CnZGIpGUF+wcLnP3TMcwYlVUVPj69eszHYaISL9aWlqoq6tj4sSJhEIhpkyZkumQMLMN7l6RaN24mEggIjLWuDuNjY3s3LkzrQU7h0tJR0RklIkv2Dlv3jyWLFkyIq/fJKKkIyIyimS6YOdwKemIiIwS+/bto6amBjNj2bJl5OXlZTqkIVPSEREZBXbu3ElDQwNTpkwhFAqRk5OT6ZCOi5KOiMgI1t3dTW1tLa2trcyePZulS5dmvH7acCjpiIiMUJ2dnYTDYQ4dOkRBQQELFiw49odGOCUdEZERaKQW7BwuJR0RkRGmubmZ7du3M2nSJEKh0IirnzYcSjoiIiNET8HO3bt3M2PGDIqLi8nKysp0WEmlpCMiMgJ0dXVRXV094gt2DpeSjohIhsUX7CwpKWHWrFmZDilllHRERDIovmBneXn5iCjYmUpKOiIiGeDuNDQ00NTURF5eHqFQiIkTx/6v5LG/hyIiI0w0GqW6upr9+/ePuoKdw6WkIyKSRocPH6aqqorOzk6WLl3K3LlzMx1SWinpiIikyVgo2DlcSjoiImkwVgp2DpeSjohICo21gp3DpaQjIpIinZ2dVFVV0d7ezuLFi8nPz890SBmnpCMikgIHDhyguroad6esrIzp06dnOqQRQUlHRCTJdu3aRX19/Zgs2DlcSjoiIkkyHgp2DpeSjohIEnR1dREOhzl48CALFy5k0aJFmQ5pRFLSEREZpoMHDxIOh4lGo2O+YOdwKemIiAzDnj17qKurIzs7m+XLl5Obm5vpkEY0JR0RkeMQX7Bz2rRplJSUjIuCncOlIyQiMkSRSISamhr279/P/PnzWbx48bgp2DlcSjoiIkPQ3t5OOBwetwU7h0tJR0RkkPbu3UtNTQ1ZWVnjtmDncCnpiIgMwo4dO2hsbBz3BTuHS0lHRGQA3d3d1NTUsHfvXubMmUNhYeG4Ltg5XEo6IiL96OjoIBwOq2BnEinpiIgkoIKdqaGkIyLSR3zBztLSUiZNmpTpkMYMJR0RkYC7U1dXx549e5g5cyZFRUUq2JlkSjoiIqhgZ7oo6YjIuBdfsDMUCjFz5sxMhzRmKemIyLjWU7AzJyeHsrIyFexMMSUdERmX3J36+np27dqlgp1ppCMsIuNOJBKhurqaAwcOqGBnminpiMi4El+ws6ioiDlz5mQ6pHFFSUdExo34gp3l5eVMnTo10yGNO0o6IjIuNDY2smPHDqZOnUooFCI7OzvTIY1LSjoiMqZFo1Fqa2tVsHOEUNIRkTGrp2Dn4cOHWbJkCfPnz890SOOeko6IjEn79++nuroaM6OsrIxp06ZlOiQBMnqOaWbnmdkWM6sys68kWD/JzO4O1j9nZkVx674atG8xs3OP1aeZ3RG0bzKzW81MA7oiY1RTUxOVlZXk5OSwfPlyJZwRJGNJx8yygJ8A5wMrgUvNbGWfzT4BtLp7KXAT8P3gsyuBS4ATgPOAn5pZ1jH6vANYDpwE5AKfTOHuiUgGdHd3U1tbS319PTNnzmT58uWqED3CZPJM581AlbtXu3snsBa4sM82FwK3Bcu/Bc622B1cFwJr3b3D3WuAqqC/fvt094c8ADwPLE7x/olIGnV1dbF161b27NnDokWLCIVCmjAwAmXyT6QA2B73vj5oS7iNu0eAfcCcAT57zD6DYbXLgYeHvQciMiIcPHiQ1157jfb2dkKhEAsXLsx0SNKP8TiR4KfAk+6+LtFKM7sauBqgsLAwnXGJyHHYvXs327ZtU8HOUSKTZzoNwJK494uDtoTbmNlEYAawZ4DPDtinmX0TmAf8Y39Bufsv3L3C3SvmzZs3xF0SkXRxd7Zv305dXR3Tpk1j+fLlSjijQCaTzl+BMjMrNrMcYhMDHuyzzYPAlcHyh4DHg2syDwKXBLPbioEyYtdp+u3TzD4JnAtc6u7dKd43EUmhSCRCZWUlu3btIj8/n9LSUlWIHiUy9qfk7hEz+zzwCJAF3Orur5rZjcB6d38QuAX4tZlVAS3EkgjBdvcAm4EI8Dl3jwIk6jP4yp8DdcAzQTXZ+9z9xjTtrogkSXt7O1VVVXR1dalg5yhksRMHSaSiosLXr1+f6TBEJNDa2kptbS1ZWVmEQiEV7ByhzGyDu1ckWqfzUREZFVSwc2xQ0hGRES2+YOfcuXMpLCzUA9dGMSUdERmxOjo6qKqqoqOjQwU7xwglHREZkVSwc2xS0hGREaepqYn6+npyc3MJhUKqnzaG9Hufjpn1FNf8cPrCEZHxrLu7m5qaGurr65k1a5YKdo5BA90cekFQXPOr6QpGRMavzs5OtmzZQktLC4sWLaKkpEQFO8eggYbXHgZagTwz2x/XboC7+/SURiYi40ZbWxvV1dV0d3cTCoWYOXNmpkOSFBnovxH/7O4zgT+4+/S41zQlHBFJlt27d7N161YmTJjA8uXLlXDGuIHOdJ4BTgX2D7CNiMhx6SnY2dzczPTp0ykpKSErKyvTYUmKDZR0cszso8BbzOwDfVe6+32pC0tExrJIJEJ1dTUHDhwgPz+fgoIC3fA5TgyUdD4DXAbMBN7XZ50DSjoiMmSHDh0iHA4TiUQoLi5m9uzZmQ5J0qjfpOPuTwFPmdl6d78ljTGJyBjVU7Bz4sSJlJeXM2XKlEyHJGnWb9Ixs7Pc/XGgVcNrIjIc7k5jYyM7d+4kLy+PkpISFewcpwYaXnsH8DhHD62BhtdEZJCi0Sg1NTXs27dPBTtlwOG1bwY/P56+cERkLDl8+DDhcJiOjg4KCwsZzY+Af3RzE+sqmzmzbB7nrMzPdDij1kDDa/840Afd/V+TH46IjBX79u2jpqZmTBTsfHRzE9fe9SLtXVHuXV/PzZeeosRznAa6OXRa8KoArgEKgtdniN2/IyKS0M6dO6mqqmLSpEmsWLFiVCccgHWVzbR3RQFo74qyrrI5wxGNXgMNr30LwMyeBE519wPB+xuAP6QlOhEZVbq7u6mrq6OlpYVZs2ZRVFQ0JuqnnVk2j3vX19PeFSU3O4szy0bvMGGmDebRBvlAZ9z7zqBNRKRXZ2cn4XCYQ4cOUVBQwIIFCzIdUtKcszKfmy89Rdd0kmAwSed24Hkzuz94fxHwq1QFJCKjT1tbG+FwGHentLSUGTNmZDqkpDtnZb6STRIcM+m4+3fM7I/AmUHTx939xdSGJSKjRXNzM9u3b2fSpEmEQiEmT56c6ZBkBBvUk0Pd/QXghRTHIiKjSHzBzhkzZlBcXKyCnXJMely1iAxZJBIhHA7T1tbGggULWLRokW74lEFR0hGRIVHBThmOY85lNLOpZjYhWF5mZu83MxVNEhmHWlpa2LJlCwDl5eVKODJkg5lA/yQw2cwKgP8HXI5mr4mMK+5OQ0MDNTU1TJkyhRUrVqhCtByXwQyvmbsfMrNPAD919x+Y2cYUxyUiI0R8wc558+axZMkSXb+R4zaopGNmZxB7oNsngjZNUREZBw4fPkxVVRWdnZ2jvmDnQFTMM30Gk3T+AfgqcL+7v2pmJcATKY1KRDIuvmDnsmXLyMvLy3RIKTFQMU8lo+QbzM2h/wP8D0AwoWC3u1+b6sBEJHN27txJQ0MDU6ZMIRQKkZOTk+mQUiZRMc9zVuarsnSKDGb22p1mNt3MpgKbgM1m9k+pD01E0q27u5vq6moaGhqYPXs25eXlYzrhQKyYZ2527IpBfDFPVZZOjcHMXlvp7vuJ1Vz7I1BMbAabiIwhnZ2dbNmyhdbWVhYvXkxxcfGYqBB9LD3FPK84Y+kRZzP9JSMZnsFc08kO7su5CPixu3eZmac2LBFJpwMHDlBdXT2mC3YOJFExT1WWTo3BJJ3/AGqBl4AnzWwpsD+VQYlI+qhgZ/9UWTr5BjOR4Gbg5rimOjN7V+pCEpF0cHe2bdvG7t27VbBT0uaYScfMZgDfBN4eNP0PcCOwL4VxiUgKdXV1UV1dndKCnZpuLIkMZnjtVmKz1v42eH858F/AB1IVlIikzqFDh6iqqiIajVJSUsKsWbOS2v+jm5u487k6nq7aQ2e0+4jpxokSkZLT+GLuA88JMLON7r76WG1jUUVFha9fvz7TYYgkTUtLC3V1dUycOJFQKJT0+mnx97bEK8/P490rF3DrUzW0d0XJzc7i5ktPAejdvqdNiWf0M7MN7l6RaN1gznTazext7v5U0NlbgfZkBigiqdVTsLOpqYlp06ZRUlLCxInJe7JJz9nK9pZDRyUcgC1NbVTtqiIa/B83/r6XRDdmytg1mL911wC3Bdd2DGgBPpbKoEQkeaLRKNXV1ezfv/+4CnYea0gM3jhbmTjBMINEAyhRh6wJRrTbe+972bh97xHbTJuc3e93ytgwmNlrG4GTzWx68F7TpUVGifiCnUuXLmXu3LlD+nyiUjDwRpK547ltrFg4vfdsJdJ9dLbpSUI5WRP41NtLOHC4qzeZ9L3L/3+2NrN6yUw+d8cLdEa7Wfv8dn5y2alKPGNIv0nHzP6xn3YA3P1fUxSTiCTB3r17qa2tZcKECcddsLNvKZg7n6tjyewpvW3Rbmdzwz5ysibQGe1O2EfPWU+kO7b+zLJ5vcnmzLJ53Pnctt5ktalhH9/+7829fXVGu7nzuTolnTFkoDOdaWmLQkSSaseOHTQ2Ng67YOeZZfNY+/z23iTwdNUe3r3yyF8b3cBbS+f0ru+MdjPBoO9JT7fDz56oYmKQoHrOnPKnTaJh3+He7Xa3HUbGrn6Tjrt/K52BiMjwdXd3U1tbS2trK7Nnz2bp0qXDqp92zsp83lo6hye2xM5MOqPdhHcdOGKbrAnGR09fesSU6GmTs3tnqh0RX9AHvDFx4KJTF/OTJ6p6t3lHeT6PbW6iM9pNTtYEPnr60uOOX0ae5E1fEZGM6ujoIBwO097ezuLFi8nPT86Q1EdPX8qz1S2905rfvXIB21piCSXL4DPvCPUOf8WXjVm9ZCbrKpvZ3dbJI5t2EA2u60As8fRMJujZ/rHNO3n3ygX807nlmkgwhh3zPp3xTPfpyGgRX7CzpKSE6dOnJ7X/vklgqEmh72w3JZSxbaD7dDKadMzsPOD/EHv89S/d/Xt91k8CbgdOA/YAH3H32mDdV4k9PjsKXOvujwzUp5kVA2uBOcAG4HJ37xwoPiUdGQ127dpFfX09kyZNorS0lEmTJmU6JBnnjuvm0P5mr/UY7uw1M8sCfgKcA9QDfzWzB919c9xmnwBa3b3UzC4Bvg98xMxWApcAJwCLgMfMbFnwmf76/D5wk7uvNbOfB33/bDj7IJJJ8QU7Z86cSVFRkQp2yog30BXGacGrgtgNogXB6zPAqUn47jcDVe5eHZxxrAUu7LPNhcBtwfJvgbMtNmf7QmCtu3e4ew1QFfSXsM/gM2cFfRD0eVES9kEkI7q6utiyZQu7d+9m4cKFhEIhJRwZFY45e83MngROdfcDwfsbgD8k4bsLgO1x7+uB0/vbxt0jZraP2PBYAfBsn88WBMuJ+pwD7HX3SILtRUaVgwcPEg6HU1awUySVBjN7LR+Iv/bRGbSNSWZ2NXA1QGFhYYajETnSnj17qKurIzs7m+XLl5Obm5vpkESGZDBJ53bgeTO7P3h/EW8MeQ1HA7Ak7v3ioC3RNvVmNhGYQWxCwUCfTdS+B5hpZhODs51E3wWAu/8C+AXEJhIMfbdEki/VBTtF0uWYd425+3eAjwOtwevj7v6/k/DdfwXKzKzYzHKITQx4sM82DwJXBssfAh732HS7B4FLzGxSMCutDHi+vz6DzzwR9EHQ5wNJ2AeRlItEIlRVVdHU1MT8+fMpKytTwpFRa7B/c6cA+939v8xsnpkVBxfwj1twjebzwCPEpjff6u6vmtmNwHp3fxC4Bfi1mVURq259SfDZV83sHmAzEAE+5+5RgER9Bl95PbDWzL4NvBj0LTKitbe3Ew6Hj7tgp8hIM5iHuH2T2Ay2cndfZmaLgHvd/a3pCDCTdJ+OZNLevXupqakhKyuLUCjE1KlTMx2SyKAM9yFuFwOnAC8AuHujmakYqEgK9RTsnDp1KqFQiOzs7EyHJJIUg0k6ne7uZuYAZqb/bomkSDQapba2lr179zJnzhwKCwuHVbBTZKQZTNK5x8z+g9jsr08BVwG/TG1YIuNPfMHOJUuWMH/+/EyHJJJ0g3ly6I/M7BxgP1AOfMPdH015ZCLjSHzBzrKysqQX7BQZKY6ZdMzs++5+PfBogjYRGaaegp2TJ08mFAqpYKeMaYMZLD4nQdv5yQ5EZLzpeeDa9u3bmTFjBuXl5Uo4MuYNVGX6GuCzQImZvRy3ahrwdKoDExnLurq6CIfDHDx4kEWLFrFw4cJMhySSFgMNr90J/BH4LvCVuPYD7t6S0qhExrD4gp2hUIiZM2dmOiSRtBmoyvQ+YB9wKYCZzQcmA3lmlufu29ITosjY0VOwMycnh7KyMhXslHFnMBMJ3gf8K7GHpe0ClgKvEXuAmogMgrtTX1/Prl27mD59OsXFxaqfJuPSYCYSfBtYA2x192LgbI58lo2IDCASiVBZWcmuXbuYP38+paWlSjgybg0m6XS5+x5ggplNcPcniNViE5FjaG9v5/XXX6etrY2ioiKWLFlC7EG2IuPTYP67tdfM8oAngTvMbBdwMLVhiYx+ra2t1NbWkpWVRXl5uQp2ijC4pHMhcBi4DriM2IPUbkxlUCKjXWNjIzt27FDBTpE+BlMG5yCAmU0H/m/KIxIZxR7ZtIOHXqhmxSw476QCFewU6WMws9c+DXyL2NlON2CAAyWpDU1kdHnopXquu+dlOqLOIxMnEAqVUjTMhPPo5ibWVTZzZtk8zlmZ39t253N1AHz09KW97SKjwWCG174EnOjuu1MdjMhotX//fv77r1V0RGMPRTwc6WZdZTPnrMxPmDgG49HNTVx714u0d0W5d309N196CgCfu+MFOqPdADxdtYefXHaqEo+MGoNJOmHgUKoDERmtmpqaqK+vp2LJVB6va+dwVze52VmcWTbvqMRx1duKOXC466gElCgx3flcHe1dUQDau6Ksq2wG6E04Pcs9yU1kNBhM0vkq8Bczew7o6Gl092tTFpXIKNDd3c22bdvYs2cPs2bN4mOnFLFkSfMRyeMbD2w6InH87IkquoE7n9vGmWVz+ejpSwESntE8XbWn97tysiZwZtk8ANY+v7038cS3i4wGg0k6/wE8DrxC7JqOyLjX2dlJOBzm0KFDRxTsPGdl/hFnHdMmHzlrrecfUKTbeWJLM89Wt7CmZPYxz2jeWjqnt9+fXHaqrunIqDWYpJPt7v+Y8khEUuR4r6n0p62tjerq6t6CnX9t7OBnz29KOGT22OadA/bV3hVld1sHudlZtHdFe4flAO5dX9/b1nNGBEcnNpHRxNx94A3M/jdQS2y6dPzw2pivNF1RUeHr16/PdBgyDPHXVHKzs7j50lOG9Qt79+7dbNu2jZycHEKhEE/V7E/Yf/z39sjJmkC3O5HuI//N5WRN4FNvLznqWk+yk6VIupjZBndPWLlmMGc6lwY/vxrXpinTMiqsq2w+aujqeH6B91ewc11lOGH/8d8LUJ6fx5fOXd4b04vbWnmlYT8QG0Y7cLiLGy888Yjv1BmNjEXHvInA3YsTvJRwZFQ4s2weudlZAEcMXQ1FfMHO/Pz8Iwp29u1/2uRsvvHAJqZNzj6i/UvnLu9NIjdeeCLXnr1s2HGJjEb9Dq+Z2Vnu/riZfSDRene/L6WRjQAaXhsbhjNM1d7eTlVVFV1dXRQVFTF79ux++582OZtbn6rpHWrrb3p0MuISGcmOd3jtHcRmrb0vwToHxnzSkbHheIepBluws6f/vtOjEw2ZJSMukdFsoCeHfjNYvNHda+LXmVlxSqMSybDGxkYe3FDD5j3OBacWs2oQFaLPLJt3xIwzDZmJHG0wEwl+B5zap+23wGnJD0cks6LRKDU1NTz2WhM3PbePjojzaPgVsiZOPOZZyTkr87n50lM0ZCYygH6TjpktJ/ZI6hl9rutMByanOjCRdDt8+DDhcJiOjg7CB3PoiMSudw5l1puGzEQGNtCZTjnwXmAmR17XOQB8KoUxiaTd/v37qa6uxswoKyvjbyYf4r9f3dM7VNYzK01nMCLDM5ibQ89w92fSFM+Iotlr40NPwc7c3FxKS0vJyckB4IePbOGxzTsJzZ/GE6/vStoNpiJj3XBvDr3YzF4F2oGHgVXAde7+myTGKJI2PVOV31Y6h7Iph2lpaWHWrFkUFRX1PnDt0c1NvdOfq3a1ETyxYFg3mIrIIG4OBf7G3fcTG2qrBUqBf0plUCKp0lOe5vZn6vjCnS/yyKZGCgoKKCkpOeIJn/EVBaIOWRMM0I2cIsM1qIKfwc/3APe6+z4zS2FIIqkTn0w6ok7t4VwWLFhw1HZ9pz8f60bPeLrpU6R/g0k6/9fMXic2vHaNmc0j9uhqkVHn5PxJ5GRBZxQmZ0/g7BMKEm53vNOfEz3tU4lH5A3HTDru/hUz+wGwz92jZnYIuDD1oYkkj7uzfft2irL387V3LaLqwETeXj5/wIRwPNOfk1VgVGSs6veajpl9Oe7t2e4eBXD3g4CeGiqjRiQSYevWrTQ3N7NgwQKuOHs1/3LxSSlJBskoMCoylg1U8PMFdz+173Ki92OVpkyPfocOHSIcDhOJRFi6dGnCgp3Jpms6Mt4d75Rp62c50XuREaelpYW6ujomTpxIeXk5U6ZMScv3qiqBSP8GSjrez3Ki9yKDko6zAHensbGRnTt3kpeXR0lJCdnZ2cf+oIik3EBJ52Qz20/srCY3WCZ4r9prMmTpmNnVU7Bz3759zJ07l8LCQjTFX2TkGOjRBlnpDETGvlTP7Iov2FlYWMi8ebqILzLSDKYigUhSpHJm1759+3j99deJRCIsW7ZMCUdkhBrMzaEiSZGq583s3LmThoYGpkyZQigU6i3YKSIjj5KOpFUyZ3Z1d3dTW1tLa2vrUQU7RWRkUtKRUaVn9tsZxTMpyj7AoUOHKCgoSFg/TURGHiUdGTXiZ7/d/fw2vviWWXzkbSuZMWNGpkMTkUHKyFiEmc02s0fNrDL4Oauf7a4Mtqk0syvj2k8zs1fMrMrMbrZgTmx//ZrZZWb2cvCZv5jZyenZU0mmvhWi6w7nKuGIjDKZGgD/CvAndy8D/hS8P4KZzQa+CZwOvBn4Zlxy+hmxR2aXBa/zjtFvDfAOdz8J+BfgF6nYKUkdd2f5TMgJJvJPzp7AO1cszGhMIjJ0mRpeuxB4Z7B8G/Bn4Po+25wLPOruLQBm9ihwnpn9GZju7s8G7bcDFwF/7K9fd/9LXL/PAouTuC+SYl1dXVRXV1Oe18GN5xWzeU83Zy5TXTOR0ShTSSff3XcEyzuBRL89CoDtce/rg7aCYLlv+2D7/QSxBCWjQHzBzuLiYk4boGCnCm2KjHwpSzpm9hiQaErR1+LfuLubWdJruSXq18zeRSzpvK2/z5nZ1cDVAIWFhckOS4ZgKAU79fA0kdEhZUnH3d/d3zozazKzhe6+w8wWArsSbNbAG0NlEBsS+3PQvrhPe0Ow3G+/ZrYK+CVwvrvvGSDuXxBc86moqFBh0wxwdxoaGmhqaiIvL49QKMTEiQP/VdXD00RGh0xNJHgQ6JmNdiXwQIJtHgH+xsxmBRMI/gZ4JBg+229ma4JZa1fEfT5hv2ZWCNwHXO7uW1OxQ5Ic0WiUqqoqmpqamDdvHsuWLTtmwgE9PE1ktOj3IW4p/VKzOcA9QCFQB/ytu7eYWQXwGXf/ZLDdVcD/Cj72HXf/r6C9AvgVkEvs+swXguG0/vr9JfDBoA0g0t8DhuLpIW7pdfjwYaqqqujs7KSwsJC5c+cO6fO6piMyMgz0ELeMJJ3RQkknffbt20dNTQ1mRigUIi8vL9MhichxOt4nh4qkhQp2iowfSjqSMfEFO2fPns3SpUtVsFNkjFPSkYzo7OykqqqK9vZ2Fi9eTH6+rsGIjAdKOpJ2Bw4coLq6GnenrKyM6dOnZzokEUkTJR1Jq127dlFfX8+kSZMIhUJMnjw50yGJSBop6UhauDvbtm1j9+7dzJgxg+LiYrKysjIdloikmZKOpFxXVxfhcJiDBw+ycOFCFi1alOmQRCRDlHQkpQ4ePEg4HCYajVJSUsKsWQkfnSQi44SSjqTMnj17qKurIzs7m+XLl5Obm5vpkEQkw5R0JOniC3ZOmzaNkpKSQdVPE5GxT78JJKkikQg1NTXs37+f+fPns3jxYoKniYuIKOlI8rS3txMOh+ns7GTp0qVDLtgpImOfko4kxd69e6mpqSErK4tly5apYKeIJKSkI8O2Y8cOGhsbVbBTRI5JSUeOW3d3NzU1Nezdu1cFO0VkUJR05Lh0dHQQDodVsFNEhkRJR4ZMBTtF5Hgp6ciQxBfsLC0tZdKkSZkOSURGESUdGRR3p66ujj179jBz5kyKiopUsFNEhkxJR45JBTtFJFmUdGRAKtgpIsmkpCP96inYmZOTQ1lZmQp2isiwKenIUdyd+vp6du3apYKdIpJU+k0iR4hEIlRXV3PgwAEV7BSRpFPSkV7xBTuLioqYM2dOpkMSkTFGSSfNHt3cxLrKZs4sm8c5K0fOXfzxBTvLy8uZOnVqpkMSkTFISSeNHt3cxLV3vUh7V5R719dz86WnjIjE09jYyI4dO5g6dSqhUIjs7OxMhyQiY5SqM6bRuspm2ruiALR3RVlX2ZzReKLRKOFwmB07djBnzhyWLVumhCMiKaWkk0Znls0jNzt2F39udhZnls3LWCwdHR1s2bKFffv2sWTJEoqKilQhWkRSTsNraXTOynxuvvSUjF/T2b9/P9XV1QCUlpaqYKeIpI2STpqdszI/o9dxmpqaqK+vJzc3l1AopIKdIpJWSjrjRHd3N9u2best2FlcXKzhNBFJOyWdcSC+YOeiRYtYuHBhpkMSkXFKSWeMiy/YGQqFmDlzZqZDEpFxTElnDNu9ezfbtm1TwU4RGTGUdMag+IKd06dPp7i4WAU7RWRE0JXkMSYSiVBZWcmuXbvIz8+ntLRUCUfGHTPj7/7u73rfRyIR5s2bx3vf+94jtrvoootYs2bNEW033HADBQUFrF69uve1d+/eYcXzkY98pLevoqIiVq9enXC7q666ivnz53PiiSce1/ds2LCBk046idLSUq699lrcHTh6nx566KHj3ZVhU9IZQ9rb23nttddoa2ujqKhIFaJl3Jo6dSqbNm2ivb0dgEcffZSCgoIjttm7dy8bNmxg3759vfet9bjuuuvYuHFj72u410Lvvvvu3r4++MEP8oEPfCDhdh/72Md4+OGHj/t7rrnmGv7zP/+TyspKKisrj+grfp8uuOCC4/6O4VLSGSNaW1t5/fXXcXfKy8tVIVrGvQsuuIA//OEPANx1111ceumlR6y/7777eN/73scll1zC2rVr0xKTu3PPPfccFUuPt7/97cyePfuo9nA4zHnnncdpp53GmWeeyeuvv37UNjt27GD//v2sWbMGM+OKK67g97//fbJ3YdiUdMaAxsZGqquryc3NZcWKFaoQLQK9yeTw4cO8/PLLnH766Ues70lEl156KXfdddcR62666abeoah3vetdR/V94MCBI4bf4l+bN2/uN6Z169aRn59PWVnZkPbl6quv5t///d/ZsGEDP/rRj/jsZz971DYNDQ0sXry49/3ixYtpaGjoff/jH/+YVatWcdVVV9Ha2jqk708mDfaPYtFolNraWvbu3cvcuXMpLCzUcJpIYNWqVdTW1nLXXXcdNZzU1NREZWUlb3vb2zAzsrOz2bRpU++1lOuuu44vfelL/fY9bdo0Nm7cOOSYEp1xHUtbWxt/+ctf+PCHP9zb1tHRMaQ+rrnmGr7+9a9jZnz961/ni1/8IrfeeuuQ+kgWJZ1RqqOjg6qqKjo6OliyZAnz58/PdEgiI8773/9+vvSlL/HnP/+ZPXv29Lbfc889tLa2UlxcDMTqEd5111185zvfGVS/Bw4c4Mwzz0y47s4772TlypVHtUciEe677z42bNgwpH3o7u5m5syZRyW5aDTKaaedBsT285prrqG+vr53fX19fe91rPz8N0pvfepTnzpqQkU6KemMQj0FO82MsrIypk2blumQREakq666ipkzZ3LSSSfx5z//ubf9rrvu4uGHH+aMM84AoKamhne/+92DTjrHc6bz2GOPsXz58iOGwAaj57aHe++9lw9/+MO4Oy+//DInn3zyUTFMnz6dZ599ltNPP53bb7+dL3zhC0Dsek9PJZL777//uGfHJYOu6YwyPcMCOTk5LF++XAlHZACLFy/m2muvPaKttraWurq6I6ZKFxcXM2PGDJ577jngyGs6q1evpra2dtixrF279qihtcbGxiOG/i699FLOOOMMtmzZwuLFi7nlllsAuOOOO7jllls4+eSTOeGEE3jggQcSfsdPf/pTPvnJT1JaWkooFOL8888H4Mtf/jInnXQSq1at4oknnuCmm24a9v4cL+uZxy1Hq6io8PXr12c6DCB2il1XV0dLSwuzZs3S829EZMQysw3uXpFonYbXRoHOzk7C4TCHDh1SwU4RGdWUdEa4trY2qqur6e7uVsFOERn1MjI+Y2azzexRM6sMfs7qZ7srg20qzezKuPbTzOwVM6sys5stmCd8rH7N7E1mFjGzD6V2D5Nj9+7dbN26lQkTJrB8+XIlHBnXsrKyWL16NSeeeCLve9/7ekvTbNy4kTPOOIMTTjiBVatWcffdd2c20GH67ne/S2lpKeXl5TzyyCMJt3F3vva1r7Fs2TJWrFjBzTffDMADDzzAqlWrWL16NRUVFTz11FO9n/nyl7/MCSecwIoVK44okZN27p72F/AD4CvB8leA7yfYZjZQHfycFSzPCtY9D6wBDPgjcP6x+gWygMeBh4APDSbO0047zTOhu7vb6+rqfP369b5161aPRCIZiUNkJJk6dWrv8hVXXOHf/va33d19y5YtvnXrVnd3b2ho8AULFnhra2vK40nFv8tXX33VV61a5YcPH/bq6movKSlJ+D233nqrX3755R6NRt3dvampyd3dDxw44N3d3e7u/tJLL3l5ebm7uz/99NP+lre8xSORiEciEV+zZo0/8cQTSY+/B7De+/m9mqkr0RcCtwXLtwEXJdjmXOBRd29x91bgUeA8M1sITHf3Z4Oduz3u8wP1+wXgd8Cu5O1G8vUU7Gxubu4t2JmVlZXpsERGlDPOOKP3bvtly5b13uG/aNEi5s+fT3Nz81Gfufnmm1m5ciWrVq3ikksuAWLD1x//+Md7Z3b97ne/A2JTqk866SROPPFErr/++t4+8vLy+OIXv8jJJ5/MM888w29+8xve/OY3s3r1aj796U8TjUaHtV8PPPAAl1xyCZMmTaK4uJjS0lKef/75o7b72c9+xje+8Y3eyUQ99+nl5eX13iB+8ODB3mUz4/Dhw3R2dtLR0UFXV9cR9+6kU6aSTr677wiWdwKJ9r4A2B73vj5oKwiW+7b326+ZFQAXAz9LSvQpcujQod6CncXFxSrYKZJANBrlT3/6E+9///uPWvf888/T2dlJKBQ6at33vvc9XnzxRV5++WV+/vOfA/Av//IvzJgxg1deeYWXX36Zs846i8bGRq6//noef/xxNm7cyF//+tfeGmYHDx7k9NNP56WXXmLOnDncfffdPP3002zcuJGsrCzuuOOOo773uuuuS1gu53vf+95R2zY0NLBkyZLe931L2fQIh8PcfffdVFRUcP7551NZWdm77v7772f58uW85z3v6a06cMYZZ/Cud72LhQsXsnDhQs4991xWrFhxjCOdGimbSGBmjwELEqz6Wvwbd3czS/rgYp9+/w243t27j/VL3MyuBq4GKCwsTHZY/WptbaW2tpaJEyeyfPlypkyZkrbvFhkN2tvbWb16NQ0NDaxYsYJzzjnniPU7duzg8ssv57bbbkt4O8GqVau47LLLuOiii7jooouA2A2b8cU+Z82axZNPPsk73/lO5s2bB8Bll13Gk08+yUUXXURWVhYf/OAHAfjTn/7Ehg0beNOb3tQbX6LKIKm4J6ajo4PJkyezfv167rvvPq666irWrVsHwMUXX8zFF1/Mk08+yde//nUee+wxqqqqeO2113orFpxzzjmsW7eu36oKqZSyMx13f7e7n5jg9QDQFAyTEfxMNOTVACyJe784aGsIlvu2M0C/FcBaM6sFPgT81Mwu6ifuX7h7hbtX9PylSyV3p6GhgerqaqZMmaKEI9KP3NxcNm7cSF1dHe7OT37yk951+/fv5z3veQ/f+c53jno+To8//OEPfO5zn+OFF17gTW96E5FIZMgxTJ48uXe429258sorex8XsGXLFm644YajPjOUM52CggK2b39jgCe+lE28xYsX9z4e4eKLL+bll18+apu3v/3tVFdXs3v3bu6//37WrFlDXl4eeXl5nH/++TzzzDND3v+k6O9iTypfwA858oL/DxJsMxuoITaJYFawPNsTTyS4YAj9/ooRMpEgEol4ZWWlr1+/3mtra3svAIrI0eInErzwwgteWFjoXV1d3tHR4WeddZbfdNNN/X42Go16TU2Nu7t3dnb6woULvbW11a+//nr/+7//+97tWlpavLGx0QsLC725udkjkYifffbZ/vvf//6oGF599VUvLS3tvYi/Z88er62tHdY+btq06YiJBMXFxQknElx//fV+yy23uLv7E0884RUVFe7uXllZ2ft7ZMOGDb5o0SLv7u72tWvX+tlnn+1dXV3e2dnpZ511lj/44IPDinUgDDCRIFNJZw7wJ6ASeCwumVQAv4zb7iqgKnh9PK69AtgEhIEf80ZlhYT99vnuEZF02tvbfdOmTb5hwwbftWtXyr5HZKyI/4Xv7v7e977Xb7/9dv/1r3/tEydO9JNPPrn39eKLLx6xbWdnp7/1rW/1E0880U844QT/7ne/6+6x2V5XXHGFn3DCCb5q1Sr/3e9+5+7ud955Z++2X/7yl/uNYe3atX7yySf7SSed5Keeeqo/88wzw97Pb3/7215SUuLLli3zhx56qLf9/PPP94aGBnd3b21t9QsuuMBPPPFEX7NmjW/cuNHd3b/3ve/5ypUr/eSTT/Y1a9b4unXr3D32H9yrr77aly9f7itWrPDrrrtu2HEOZKCkozI4A0hVGZx9+/ZRU1ODmVFSUqL6aSIypqgMzgiyc+dOGhoayM3NpbS0lJycnEyHJCKSNko6aaKCnSIiSjppEV+ws6CggAULEs0kFxEZ+5R0UqytrY1wOIy7U1payowZMzIdkohIxijppFBzczPbt29n0qRJhEIhJk+enOmQREQySkknBdyd7du309zczIwZMyguLlb9NBERlHSSLhKJEA6HaWtrY8GCBSxatEj100REAko6SXTo0CHC4TCRSITi4mJmz56d6ZBEREYUJZ0kaWlpoa6ujokTJ1JeXq76aSIiCSjpDJO709jYyM6dO8nLyyMUCjFxog6riEgi+u04DNFolJqaGvbt28e8efNYsmSJrt+IiAxASec4HT58mHA4TEdHB4WFhaTjMQgiIqOdks5xiC/YuWzZMvLy8jIdkojIqKCkM0Q9BTunTJlCKBRSwU4RkSFQ0hmk7u5uamtraW1tZfbs2SxdulQFO0VEhkhJZxBUsFNEJDmUdI7hwIEDVFdXq2CniEgSKOkMIBKJUFlZqYKdIiJJoqQzgM7OTqZPn66CnSIiSWLunukYRiwzawbq0vy1c4Hdaf7OkU7HJDEdl6PpmBwtE8dkqbsnvHlRSWeEMbP17l6R6ThGEh2TxHRcjqZjcrSRdkw051dERNJGSUdERNJGSWfk+UWmAxiBdEwS03E5mo7J0UbUMdE1HRERSRud6YiISNoo6aSQmc02s0fNrDL4Oauf7a4Mtqk0syvj2k8zs1fMrMrMbrbgYT3H6tfM3mRmETP7UGr3cOjSfUzM7DIzezn4zF/M7OT07Omxmdl5ZrYl2JevJFg/yczuDtY/Z2ZFceu+GrRvMbNzj9WnmRUHfVQFfY7ISrVpPiZ3BO2bzOxWM8tO+Q4eh3Qek7j1N5tZW0p2yN31StEL+AHwlWD5K8D3E2wzG6gOfs4KlmcF654H1gAG/BE4/1j9AlnA48BDwIcyfQwyfUyAt8R99nzguUwfg7g/pzBQAuQALwEr+2zzWeDnwfIlwN3B8spg+0lAcdBP1kB9AvcAlwTLPweuyfQxGAHH5ILg75EBd+mY9PZXAfwaaEvFPulMJ7UuBG4Llm8DLkqwzbnAo+7e4u6twKPAeWa2EJju7s967G/C7XGfH6jfLwC/A3YlbzeSKq3HxN3/EvQB8CywOKl7c/zeDFS5e7W7dwJrie1DvPh9+i1wdnBmdyGw1t073L0GqAr6S9hn8Jmzgj6g/+OeaWk7JgDu/pAHiP1nZqT83YiX1mNiZlnAD4Evp2qHlHRSK9/ddwTLO4H8BNsUANvj3tcHbQXBct/2fvs1swLgYuBnSYk+NdJ6TPr4BLGzo5Ggv31MuI27R4B9wJwBPttf+xxgb9BHf981EqTzmPQKhtUuBx4e9h4kX7qPyeeBB+P+LSWdaq8Nk5k9BiR61sHX4t+4u5tZ0qcK9un334Dr3b07uNSRESPsmPTE9C5iSedtyf4+GfV+Cjzp7usyHUgmmdki4MPAO1P5PUo6w+Tu7+5vnZk1mdlCd98RDA0lGvJq4Mg/5MXAn4P2xX3aG4Ll/vqtANYGCWcucIGZRdz990PesWEYYccEM1sF/JLY9Z89x7FLqdAALIl7H78vfbepN7OJwAxgzzE+m6h9DzDTzCYG/xNO9F0jQTqPCQBm9k1gHvDpJMSfCuk8JqcApUBV8DtkiplVuXtpcnYlkOkLZWP5RWxsNP7i9g8SbDMbqCF2wXxWsDw7WNf3ovkFQ+j3V4zMiQRpPSZAIbGx7Ldket/77ONEYhMkinnjYu4Jfbb5HEdeIL4nWD6BIy8QVxO7ONxvn8C9HDmR4LOZPgYj4Jh8EvgLkJvpfR8px6RPvymZSJDxgzqWX8TGVf8EVAKPxf3irAB+GbfdVcEvxirg43HtFcAmYjNNfswbN/Mm7LfPd/+KkZl00npMiJ3htAIbg9f6TB+DuH25ANga7MvXgrYbgfcHy5OJJYsqYsm2JO6zXws+t4VgBl9/fQbtJUEfVUGfkzK9/yPgmESCtp6/G9/I9P5n+pj0+d6UJB1VJBARkbTR7DUREUkbJR0REUkbJR0REUkbJR0REUkbJR0REUkbJR2RITCzi8zMzWz5ILb9BzObMozv+piZ/Xiw7ckW/z1m9hkzuyJYXm5mG83sRTMLmdm1Zvaamd2R6phk9FPSERmaS4Gngp/H8g/AcSedkcTdf+7utwdvLwJ+6+6nuHuYWJXjc9z9sowFKKOGko7IIJlZHrHabZ8gdud3T3uWmf0oeC7Ly2b2BTO7FlgEPGFmTwTbtcV95kNm9qtg+X3Bc1BeNLPHzCxRsdL+Yioys8eD7/2TmRUG7R8O4nnJzJ4M2k4ws+eDs5SXzawsQX8fN7OtZvY88Na49hvM7EtmdgGxZHqNmT1hZj8nduPpH83susHGLeOXaq+JDN6FwMPuvtXM9pjZae6+AbgaKAJWu3vEzGa7e4uZ/SPwLnfffYx+nwLWuLub2SeJlZX/4iBj+nfgNne/zcyuAm4mdibyDeBcd28ws5nBtp8B/o+732Gxh7hlxXcU1Kz7FnAasUrFTwAvxm/j7g8FiabN3X8UfO68Qe6niM50RIbgUmLPHiH42TPE9m7gPzx4dIC7twyx38XAI2b2CvBPxGpmDdYZwJ3B8q95o4r208CvzOxTvJFcngH+l5ldDyx19/Y+fZ0O/Nndmz32nJW7h7gfIsekpCMyCGY2m9iD0H5pZrXEksPf2tCeIRFfc2py3PK/Az9295OIVTuezDC5+2eAfyZWTXiDmc1x9zuB9wPtwENmdtZwv0dkqJR0RAbnQ8Cv3X2puxe5+xJi1a/PJPZk008HZeV7EhTAAWBaXB9NZrbCzCYQe9hejxm8UXL+yiHG9RfeuL50GbAuiCHk7s+5+zeAZmCJmZUA1e5+M/AAsKpPX88B7zCzOcGDzT48xFhEjklJR2RwLgXu79P2u6D9l8A24GUzewn4aLD+F8DDPRMJiD1y4b+JJYr4JzPeANxrZhuAoV4X+QLwcTN7mdjTL/8+aP+hmb1iZpuC73sJ+Ftgk5ltBE4k9rjvXh57WuQNxIbhngZeG2IsIsekKtMiIpI2OtMREZG0UdIREZG0UdIREZG0UdIREZG0UdIREZG0UdIREZG0UdIREZG0UdIREZG0+f/2hAeTTOkGLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "def visualize_result(actual_loss_diff, estimated_loss_diff):\n",
    "    max_abs = np.max([np.abs(actual_loss_diff), np.abs(estimated_loss_diff)])\n",
    "    min_, max_ = -max_abs * 1.15, max_abs * 1.15\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = 6, 5\n",
    "    plt.scatter(actual_loss_diff, estimated_loss_diff, zorder=2, s=10)\n",
    "    plt.title('Loss diff')\n",
    "    plt.xlabel('Actual loss diff')\n",
    "    plt.ylabel('Estimated loss diff')\n",
    "    \n",
    "    range_ = [min_, max_]\n",
    "    plt.plot(range_, range_, 'k-', alpha=0.2, zorder=1)\n",
    "    text = 'MAE = {:.03}\\nR2 score = {:.03}'.format(mean_absolute_error(actual_loss_diff, estimated_loss_diff),\n",
    "                                                    r2_score(actual_loss_diff, estimated_loss_diff))\n",
    "    plt.text(max_abs, -max_abs, text, verticalalignment='bottom', horizontalalignment='right')\n",
    "    plt.xlim(min_, max_)\n",
    "    plt.ylim(min_, max_)\n",
    "    plt.show()\n",
    "    \n",
    "visualize_result(loss_diff_true, loss_diff_approx[sample_indice])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
